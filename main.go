package main

import (
	"bufio"
	"fmt"
	"os"
	"strconv"
	"strings"
	"time"
)

func main() {

	reader := bufio.NewReader(os.Stdin)

	fmt.Print("Mode [scan/headless/combo] (default scan): ")
	modeStr, _ := reader.ReadString('\n')
	mode := strings.ToLower(strings.TrimSpace(modeStr))
	if mode == "" {
		mode = "scan"
	}

	if mode == "headless" || mode == "combo" {
		fmt.Println("\n" + strings.Repeat("âš ", 30))
		fmt.Println("âš ï¸  WARNING: Headless mode uses significant resources")
		fmt.Println("âš ï¸  - RAM: ~150-300MB per browser instance")
		fmt.Println("âš ï¸  - CPU: High load during page rendering")
		fmt.Println("âš ï¸  - Time: ~2-5 seconds per page")
		if mode == "combo" {
			fmt.Println("âš ï¸  - COMBO: Will run BOTH scan + headless sequentially")
		}
		fmt.Println(strings.Repeat("âš ", 30))

		fmt.Print("\nContinue? (y/n): ")
		confirm, _ := reader.ReadString('\n')
		if strings.ToLower(strings.TrimSpace(confirm)) != "y" {
			fmt.Println("Aborted.")
			return
		}
	}

	switch mode {
	case "headless":
		runHeadlessMode(reader)
	case "combo":
		runComboMode(reader)
	default:
		runScanMode(reader)
	}
}

func runScanMode(reader *bufio.Reader) {
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println("GoCrawUz - Advanced URL Discovery Tool")
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println()

	fmt.Print("Target URL: ")
	targetURL, _ := reader.ReadString('\n')
	targetURL = strings.TrimSpace(targetURL)

	if targetURL == "" {
		fmt.Println("âŒ URL required!")
		return
	}

	if !strings.HasPrefix(targetURL, "http://") && !strings.HasPrefix(targetURL, "https://") {
		targetURL = "https://" + targetURL
	}
	if strings.HasPrefix(targetURL, "http://") {
		targetURL = strings.Replace(targetURL, "http://", "https://", 1)
	}

	fmt.Print("Concurrency (default 50): ")
	threadsStr, _ := reader.ReadString('\n')
	threadsStr = strings.TrimSpace(threadsStr)

	threads := 50
	if threadsStr != "" {
		if t, err := strconv.Atoi(threadsStr); err == nil && t > 0 {
			threads = t
		}
	}

	fmt.Print("Rate limit (ms between requests, default 0): ")
	rateStr, _ := reader.ReadString('\n')
	rateLimitMs := 0
	if r, err := strconv.Atoi(strings.TrimSpace(rateStr)); err == nil && r >= 0 {
		rateLimitMs = r
	}

	fmt.Print("Enable random User-Agent? (y/n, default n): ")
	uaStr, _ := reader.ReadString('\n')
	randomUA := strings.ToLower(strings.TrimSpace(uaStr)) == "y"

	fmt.Print("Enable SPA route detection? (y/n, default y): ")
	spaDetect, _ := reader.ReadString('\n')
	enableSPA := strings.ToLower(strings.TrimSpace(spaDetect)) != "n"

	fmt.Print("Verbose mode? (y/n, default n): ")
	verboseStr, _ := reader.ReadString('\n')
	verbose := strings.ToLower(strings.TrimSpace(verboseStr)) == "y"

	fmt.Print("Enable BFS auto-crawl? (y/n, default n): ")
	bfsStr, _ := reader.ReadString('\n')
	enableBFS := strings.ToLower(strings.TrimSpace(bfsStr)) == "y"

	bfsDepth := 0
	bfsMaxURLs := 0
	if enableBFS {
		fmt.Print("BFS max depth (default 2): ")
		depthStr, _ := reader.ReadString('\n')
		depthStr = strings.TrimSpace(depthStr)
		if depthStr != "" {
			if d, err := strconv.Atoi(depthStr); err == nil && d > 0 {
				bfsDepth = d
			} else {
				bfsDepth = 2
			}
		} else {
			bfsDepth = 2
		}

		fmt.Print("BFS max URLs to visit (default 100): ")
		maxStr, _ := reader.ReadString('\n')
		maxStr = strings.TrimSpace(maxStr)
		if maxStr != "" {
			if m, err := strconv.Atoi(maxStr); err == nil && m > 0 {
				bfsMaxURLs = m
			} else {
				bfsMaxURLs = 100
			}
		} else {
			bfsMaxURLs = 100
		}
	}

	fmt.Print("Enable response analysis? (y/n, default n): ")
	analysisStr, _ := reader.ReadString('\n')
	enableAnalysis := strings.ToLower(strings.TrimSpace(analysisStr)) == "y"

	fmt.Println()
	fmt.Println(strings.Repeat("=", 60))
	fmt.Printf("ğŸ¯ Target: %s\n", targetURL)
	fmt.Printf("âš¡ Threads: %d\n", threads)
	fmt.Printf("â±ï¸  RateLimit ms: %dms\n", rateLimitMs)
	fmt.Printf("ğŸ” SPA Detection: %v\n", enableSPA)
	fmt.Printf("ğŸ“ Verbose: %v\n", verbose)
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println()

	scanner := NewScanner(targetURL, threads, enableSPA, verbose, enableAnalysis, rateLimitMs, randomUA)

	results, err := scanner.Scan()

	if err != nil {
		fmt.Printf("âŒ Error: %v\n", err)
		return
	}

	displayResults(results, scanner, enableSPA)

	if enableBFS {
		results = runBFS(scanner, targetURL, results, bfsDepth, bfsMaxURLs, rateLimitMs)
	}

	saveResultsWithDedup(reader, results, scanner, enableSPA)
}

func runHeadlessMode(reader *bufio.Reader) {
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println("GoCrawUz - Headless Browser Mode")
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println()

	fmt.Print("Target URL: ")
	targetURL, _ := reader.ReadString('\n')
	targetURL = strings.TrimSpace(targetURL)

	if targetURL == "" {
		fmt.Println("âŒ URL required!")
		return
	}

	if !strings.HasPrefix(targetURL, "http://") && !strings.HasPrefix(targetURL, "https://") {
		targetURL = "https://" + targetURL
	}

	fmt.Print("Max pages to crawl (default 50): ")
	maxPagesStr, _ := reader.ReadString('\n')
	maxPages := 50
	if p, err := strconv.Atoi(strings.TrimSpace(maxPagesStr)); err == nil && p > 0 {
		maxPages = p
	}

	fmt.Print("Enable deep mode (XHR/fetch capture)? (y/n, default y): ")
	deepStr, _ := reader.ReadString('\n')
	enableDeep := strings.ToLower(strings.TrimSpace(deepStr)) != "n"

	fmt.Println()
	fmt.Println(strings.Repeat("=", 60))
	fmt.Printf("ğŸ¯ Target: %s\n", targetURL)
	fmt.Printf("ğŸ“„ Max Pages: %d\n", maxPages)
	fmt.Printf("ğŸ” Deep Mode: %v\n", enableDeep)
	fmt.Println(strings.Repeat("=", 60))

	scanner := NewHeadlessScanner(targetURL, maxPages, enableDeep)
	results, err := scanner.Scan()

	if err != nil {
		fmt.Printf("âŒ Error: %v\n", err)
		return
	}

	allURLs := scanner.GetAllURLs(results)

	fmt.Println()
	fmt.Println(strings.Repeat("=", 60))
	fmt.Printf("âœ… Found: %d unique URLs\n", len(allURLs))
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println()

	for i, url := range allURLs {
		if i < 50 {
			fmt.Printf("  â†’ %s\n", url)
		}
	}

	if len(allURLs) > 50 {
		fmt.Printf("\n... and %d more URLs\n", len(allURLs)-50)
	}

	saveURLsWithDedup(reader, allURLs)
}

func runComboMode(reader *bufio.Reader) {
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println("GoCrawUz - COMBO Mode (Scan + Headless)")
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println()

	fmt.Print("Target URL: ")
	targetURL, _ := reader.ReadString('\n')
	targetURL = strings.TrimSpace(targetURL)

	if targetURL == "" {
		fmt.Println("âŒ URL required!")
		return
	}

	if !strings.HasPrefix(targetURL, "http://") && !strings.HasPrefix(targetURL, "https://") {
		targetURL = "https://" + targetURL
	}
	if strings.HasPrefix(targetURL, "http://") {
		targetURL = strings.Replace(targetURL, "http://", "https://", 1)
	}

	fmt.Print("Concurrency for scan (default 50): ")
	threadsStr, _ := reader.ReadString('\n')
	threads := 50
	if t, err := strconv.Atoi(strings.TrimSpace(threadsStr)); err == nil && t > 0 {
		threads = t
	}

	fmt.Print("Max pages for headless (default 30): ")
	maxPagesStr, _ := reader.ReadString('\n')
	maxPages := 30
	if p, err := strconv.Atoi(strings.TrimSpace(maxPagesStr)); err == nil && p > 0 {
		maxPages = p
	}

	fmt.Print("Rate limit (ms between requests, default 0): ")
	rateStr, _ := reader.ReadString('\n')
	rateLimitMs := 0
	if r, err := strconv.Atoi(strings.TrimSpace(rateStr)); err == nil && r >= 0 {
		rateLimitMs = r
	}

	fmt.Print("Enable random User-Agent? (y/n, default n): ")
	uaStr, _ := reader.ReadString('\n')
	randomUA := strings.ToLower(strings.TrimSpace(uaStr)) == "y"

	fmt.Print("Enable SPA route detection? (y/n, default y): ")
	spaDetect, _ := reader.ReadString('\n')
	enableSPA := strings.ToLower(strings.TrimSpace(spaDetect)) != "n"

	fmt.Print("Verbose mode? (y/n, default n): ")
	verboseStr, _ := reader.ReadString('\n')
	verbose := strings.ToLower(strings.TrimSpace(verboseStr)) == "y"

	fmt.Print("Enable response analysis? (y/n, default n): ")
	analysisStr, _ := reader.ReadString('\n')
	enableAnalysis := strings.ToLower(strings.TrimSpace(analysisStr)) == "y"

	fmt.Println()
	fmt.Println(strings.Repeat("=", 60))
	fmt.Printf("ğŸ¯ Target: %s\n", targetURL)
	fmt.Printf("âš¡ Scan Threads: %d\n", threads)
	fmt.Printf("ğŸŒ Headless Max Pages: %d\n", maxPages)
	fmt.Println(strings.Repeat("=", 60))

	// Ğ­Ñ‚Ğ°Ğ¿ 1: ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ scan
	fmt.Println("\n" + strings.Repeat("=", 60))
	fmt.Println("ğŸ“¡ STAGE 1/2: Fast HTTP Scan")
	fmt.Println(strings.Repeat("=", 60))

	scanner := NewScanner(targetURL, threads, enableSPA, verbose, enableAnalysis, rateLimitMs, randomUA)

	scanResults, err := scanner.Scan()
	if err != nil {
		fmt.Printf("âŒ Scan error: %v\n", err)
		return
	}

	fmt.Printf("\nâœ… Stage 1 complete: %d URLs found\n", len(scanResults))

	// Ğ­Ñ‚Ğ°Ğ¿ 2: Headless
	fmt.Println("\n" + strings.Repeat("=", 60))
	fmt.Println("ğŸŒ STAGE 2/2: Headless Browser Scan")
	fmt.Println(strings.Repeat("=", 60))

	headlessScanner := NewHeadlessScanner(targetURL, maxPages, true)
	headlessResults, err := headlessScanner.Scan()
	if err != nil {
		fmt.Printf("âŒ Headless error: %v\n", err)
		return
	}

	headlessURLs := headlessScanner.GetAllURLs(headlessResults)
	fmt.Printf("\nâœ… Stage 2 complete: %d URLs found\n", len(headlessURLs))

	// ĞĞ±ÑŠĞµĞ´Ğ¸Ğ½ĞµĞ½Ğ¸Ğµ Ñ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹, Ğ¿Ğ¾Ğ½ÑĞ»? ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ğ¾Ğ², Ğ½Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ±Ñ‹Ğ»Ğ¾ ĞºÑ€ÑƒÑ‚Ğ¾ Ğ½Ğ°Ğ·Ğ²Ğ°Ñ‚ÑŒ
	opts := DefaultNormalizeOptions()
	allURLs := make(map[string]bool)

	for _, r := range scanResults {
		canonical := CanonicalizeURL(r.URL, opts)
		allURLs[canonical] = true
	}

	for _, route := range scanner.spaRoutes {
		// spaRoutes ÑƒĞ¶Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ URL, Ğ½Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑÑ‚ÑŒ BaseURL
		canonical := CanonicalizeURL(route, opts)
		allURLs[canonical] = true
	}
	headlessNew := 0
	for _, url := range headlessURLs {
		canonical := CanonicalizeURL(url, opts)
		if !allURLs[canonical] {
			allURLs[canonical] = true
			headlessNew++
		}
	}

	// Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº
	finalURLs := []string{}
	for canonical := range allURLs {
		finalURLs = append(finalURLs, canonical)
	}

	fmt.Println("\n" + strings.Repeat("=", 60))
	fmt.Println("ğŸ“Š COMBO RESULTS")
	fmt.Println(strings.Repeat("=", 60))
	fmt.Printf("âœ… Total unique URLs: %d\n", len(finalURLs))
	fmt.Printf("   â””â”€ From scan: %d\n", len(scanResults)+len(scanner.spaRoutes))
	fmt.Printf("   â””â”€ New from headless: %d\n", headlessNew)
	fmt.Println(strings.Repeat("=", 60))

	fmt.Println("\nğŸ“‹ Sample URLs (first 30):")
	for i, url := range finalURLs {
		if i >= 30 {
			break
		}
		fmt.Printf("  â†’ %s\n", url)
	}

	if len(finalURLs) > 30 {
		fmt.Printf("\n... and %d more URLs\n", len(finalURLs)-30)
	}

	saveURLsWithDedup(reader, finalURLs)
}

func displayResults(results []Result, scanner *Scanner, enableSPA bool) {
	fmt.Println()
	fmt.Println(strings.Repeat("=", 60))
	fmt.Printf("âœ… Scan complete! Found: %d URLs\n", len(results))
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println()

	if len(results) == 0 {
		fmt.Println("Nothing found.")
	} else {
		byStatus := make(map[int][]Result)
		for _, r := range results {
			byStatus[r.StatusCode] = append(byStatus[r.StatusCode], r)
		}

		statuses := []int{200, 301, 302, 401, 403, 405, 500}
		for _, status := range statuses {
			if urls, ok := byStatus[status]; ok && len(urls) > 0 {
				fmt.Printf("\n[%d] Found: %d\n", status, len(urls))
				for _, r := range urls {
					note := ""
					if r.IsSPARoute {
						note = " [SPA Route]"
					}
					fmt.Printf("  â†’ %s%s\n", r.URL, note)
				}
			}
		}
	}

	if enableSPA && len(scanner.spaRoutes) > 0 {
		fmt.Println("\n" + strings.Repeat("=", 60))
		fmt.Println("ğŸ“ Extracted SPA routes by type")
		fmt.Println(strings.Repeat("=", 60))

		pages := []string{}
		apis := []string{}
		unknown := []string{}

		for _, route := range scanner.spaRoutes {
			routeType := classifyRoute(route)
			switch routeType {
			case "page":
				pages = append(pages, route)
			case "api":
				apis = append(apis, route)
			default:
				unknown = append(unknown, route)
			}
		}

		if len(pages) > 0 {
			fmt.Printf("\nğŸŒ Pages (%d) - open in browser:\n", len(pages))
			for _, route := range pages {
				fmt.Printf("  â€¢ %s\n", route)
			}
		}

		if len(apis) > 0 {
			fmt.Printf("\nğŸ”Œ API Endpoints (%d) - test with curl/Burp:\n", len(apis))
			for _, route := range apis {
				fmt.Printf("  â€¢ %s\n", route)
			}
		}

		if len(unknown) > 0 {
			fmt.Printf("\nâ“ Unknown (%d) - needs investigation:\n", len(unknown))
			for _, route := range unknown {
				fmt.Printf("  â€¢ %s\n", route)
			}
		}
	}
}

func runBFS(scanner *Scanner, targetURL string, results []Result, bfsDepth, bfsMaxURLs, rateLimitMs int) []Result {
	fmt.Println("\n" + strings.Repeat("=", 60))
	fmt.Println("ğŸ”„ Starting BFS auto-crawl")
	fmt.Println(strings.Repeat("=", 60))

	crawler := NewBFSCrawler(scanner, CrawlConfig{
		MaxDepth:  bfsDepth,
		RateLimit: time.Millisecond * time.Duration(rateLimitMs),
		MaxURLs:   bfsMaxURLs,
	})

	seedURLs := []string{targetURL}
	existingURLs := make(map[string]bool)

	for _, r := range results {
		existingURLs[r.URL] = true
	}

	for _, route := range scanner.spaRoutes {
		fullURL := scanner.BaseURL + route
		existingURLs[fullURL] = true
		seedURLs = append(seedURLs, fullURL)
	}

	bfsResults, err := crawler.CrawlMultiple(seedURLs)
	if err != nil {
		fmt.Printf("\nâš ï¸  BFS crawl error: %v\n", err)
	} else {
		newCount := 0
		for _, r := range bfsResults {
			if !existingURLs[r.URL] {
				results = append(results, r)
				newCount++
			}
		}

		fmt.Printf("\nâœ… BFS discovered %d NEW URLs (total visited: %d)\n",
			newCount, len(bfsResults))

		if newCount > 0 {
			fmt.Println("\nğŸ“ New URLs from BFS:")
			byStatus := make(map[int][]Result)
			for _, r := range bfsResults {
				if !existingURLs[r.URL] {
					byStatus[r.StatusCode] = append(byStatus[r.StatusCode], r)
				}
			}

			statuses := []int{200, 301, 302, 401, 403, 404, 405, 500}
			for _, status := range statuses {
				if urls, ok := byStatus[status]; ok && len(urls) > 0 {
					fmt.Printf("\n[%d] Found: %d\n", status, len(urls))
					for _, r := range urls {
						fmt.Printf("  â†’ %s\n", r.URL)
					}
				}
			}
		}
	}

	return results
}

func saveResultsWithDedup(reader *bufio.Reader, results []Result, scanner *Scanner, enableSPA bool) {
	fmt.Print("\nğŸ’¾ Save results? (y/n): ")
	save, _ := reader.ReadString('\n')

	if strings.ToLower(strings.TrimSpace(save)) != "y" {
		fmt.Println("\nâœ¨ Done!")
		return
	}

	// Ğ¡Ğ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²ÑĞµ URL Ñ Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸ĞµĞ¹
	opts := DefaultNormalizeOptions()
	uniqueURLs := make(map[string]string) // canonical -> original

	for _, r := range results {
		canonical := CanonicalizeURL(r.URL, opts)
		if _, exists := uniqueURLs[canonical]; !exists {
			uniqueURLs[canonical] = r.URL
		}
	}

	if enableSPA && len(scanner.spaRoutes) > 0 {
		for _, route := range scanner.spaRoutes {
			// spaRoutes ÑƒĞ¶Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ¿Ğ¾Ğ»Ğ½Ñ‹Ğµ URL
			canonical := CanonicalizeURL(route, opts)
			if _, exists := uniqueURLs[canonical]; !exists {
				uniqueURLs[canonical] = route
			}
		}
	}

	urlsFile, err := os.Create("urls.txt")
	if err != nil {
		fmt.Printf("âŒ Error: %v\n", err)
		return
	}
	defer urlsFile.Close()

	for _, url := range uniqueURLs {
		urlsFile.WriteString(url + "\n")
	}

	totalBefore := len(results)
	if enableSPA {
		totalBefore += len(scanner.spaRoutes)
	}
	removed := totalBefore - len(uniqueURLs)

	fmt.Printf("âœ… Saved to urls.txt\n")
	fmt.Printf("   â””â”€ Total: %d unique URLs", len(uniqueURLs))
	if removed > 0 {
		fmt.Printf(" (removed %d duplicates)", removed)
	}
	fmt.Println()

	fmt.Println("\nâœ¨ Done!")
}

func saveURLsWithDedup(reader *bufio.Reader, urls []string) {
	fmt.Print("\nğŸ’¾ Save results? (y/n): ")
	save, _ := reader.ReadString('\n')

	if strings.ToLower(strings.TrimSpace(save)) != "y" {
		fmt.Println("\nâœ¨ Done!")
		return
	}

	// Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ, Ñ ÑƒĞ¶Ğµ Ğ³Ğ¾Ğ²Ğ¾Ñ€Ğ¸Ğ» Ñ‡Ñ‚Ğ¾ ÑÑ‚Ğ¾ Ğ·Ğ²ÑƒÑ‡Ğ¸Ñ‚ ĞšĞ Ğ£Ğ¢Ğ??
	opts := DefaultNormalizeOptions()
	uniqueURLs := make(map[string]string)

	for _, url := range urls {
		canonical := CanonicalizeURL(url, opts)
		if _, exists := uniqueURLs[canonical]; !exists {
			uniqueURLs[canonical] = url
		}
	}

	urlsFile, err := os.Create("urls.txt")
	if err != nil {
		fmt.Printf("âŒ Error: %v\n", err)
		return
	}
	defer urlsFile.Close()

	for _, url := range uniqueURLs {
		urlsFile.WriteString(url + "\n")
	}

	removed := len(urls) - len(uniqueURLs)

	fmt.Printf("âœ… Saved to urls.txt\n")
	fmt.Printf("   â””â”€ Total: %d unique URLs", len(uniqueURLs))
	if removed > 0 {
		fmt.Printf(" (removed %d duplicates)", removed)
	}
	fmt.Println()

	fmt.Println("\nâœ¨ Done!")
}

func classifyRoute(path string) string {
	apiPatterns := []string{
		"/auth/refresh",
		"/auth/activate",
		"/auth/captcha",
		"/auth/sign-in",
		"/auth/sign-up",
		"/ping",
		"/users/stats",
		"/api/",
		"/graphql",
	}

	for _, pattern := range apiPatterns {
		if strings.HasPrefix(path, pattern) {
			return "api"
		}
	}

	pagePatterns := []string{
		"/home/",
		"/account/login",
		"/account/register",
		"/account/forgot-pass",
		"/wiki/",
		"/profile",
		"/banlist",
		"/dashboard",
		"/settings",
	}

	for _, pattern := range pagePatterns {
		if strings.HasPrefix(path, pattern) {
			return "page"
		}
	}

	return "unknown"
}
