package main

import (
	"fmt"
	"io"
	"net/http"
	"regexp"
	"sync"
	"time"
)

// –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è BFS-–∫—Ä–∞—É–ª–µ—Ä–∞
type CrawlConfig struct {
	MaxDepth  int           // –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –æ–±—Ö–æ–¥–∞
	RateLimit time.Duration // –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
	MaxURLs   int           // –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ URL –¥–ª—è –æ–±—Ö–æ–¥–∞
}

// —É–∑–µ–ª –≤ –æ—á–µ—Ä–µ–¥–∏ –æ–±—Ö–æ–¥–∞
type CrawlNode struct {
	URL   string
	Depth int
}

// –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫—Ä–∞—É–ª–µ—Ä —Å BFS-–æ–±—Ö–æ–¥–æ–º
type BFSCrawler struct {
	scanner *Scanner
	config  CrawlConfig
	queue   []CrawlNode
	visited map[string]bool
	mu      sync.Mutex
	allURLs []Result
}

// —Å–æ–∑–¥–∞—ë—Ç –Ω–æ–≤—ã–π BFS-–∫—Ä–∞—É–ª–µ—Ä
func NewBFSCrawler(scanner *Scanner, config CrawlConfig) *BFSCrawler {
	return &BFSCrawler{
		scanner: scanner,
		config:  config,
		queue:   []CrawlNode{},
		visited: make(map[string]bool),
		allURLs: []Result{},
	}
}

// Crawl –∑–∞–ø—É—Å–∫–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±—Ö–æ–¥
func (c *BFSCrawler) Crawl(startURL string) ([]Result, error) {
	opts := DefaultNormalizeOptions()
	startCanonical := CanonicalizeURL(startURL, opts)

	// –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ä—Ç–æ–≤—ã–π URL –≤ –æ—á–µ—Ä–µ–¥—å
	c.queue = append(c.queue, CrawlNode{URL: startURL, Depth: 0})
	c.visited[startCanonical] = true

	fmt.Println("\nüîÑ Starting BFS crawl...")
	fmt.Printf("üìä Max depth: %d | Rate: %v | Max URLs: %d\n\n",
		c.config.MaxDepth, c.config.RateLimit, c.config.MaxURLs)

	visitedCount := 0

	for len(c.queue) > 0 && visitedCount < c.config.MaxURLs {
		// –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –∏–∑ –æ—á–µ—Ä–µ–¥–∏
		current := c.queue[0]
		c.queue = c.queue[1:]

		if current.Depth > c.config.MaxDepth {
			continue
		}

		visitedCount++
		fmt.Printf("\rüåê Visiting [%d/%d, depth %d]: %s   ",
			visitedCount, c.config.MaxURLs, current.Depth, truncateURL(current.URL, 60))

		// –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
		links, status := c.fetchAndExtractLinks(current.URL)

		if status > 0 {
			c.mu.Lock()
			c.allURLs = append(c.allURLs, Result{
				URL:        current.URL,
				StatusCode: status,
				IsSPARoute: false,
			})
			c.mu.Unlock()
		}

		// –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –æ—á–µ—Ä–µ–¥—å
		if current.Depth < c.config.MaxDepth {
			for _, link := range links {
				abs, err := ToAbsoluteURL(link, current.URL)
				if err != nil || abs == "" {
					continue
				}

				if !IsInScope(abs, c.scanner.BaseURL, "/") {
					continue
				}

				canonical := CanonicalizeURL(abs, opts)

				c.mu.Lock()
				if !c.visited[canonical] {
					c.visited[canonical] = true
					c.queue = append(c.queue, CrawlNode{
						URL:   abs,
						Depth: current.Depth + 1,
					})
				}
				c.mu.Unlock()
			}
		}

		// –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
		if c.config.RateLimit > 0 {
			time.Sleep(c.config.RateLimit)
		}
	}

	fmt.Println("\n\n‚úÖ BFS crawl complete!")
	return c.allURLs, nil
}

// –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∏–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
func (c *BFSCrawler) fetchAndExtractLinks(urlStr string) ([]string, int) {
	req, err := http.NewRequest("GET", urlStr, nil)
	if err != nil {
		return nil, 0
	}

	resp, err := c.scanner.httpClient.Do(req)
	if err != nil {
		return nil, 0
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, resp.StatusCode
	}

	html := string(body)
	links := c.extractLinks(html)

	return links, resp.StatusCode
}

// –∏–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –∏–∑ HTML
func (c *BFSCrawler) extractLinks(html string) []string {
	links := []string{}
	seen := make(map[string]bool)

	patterns := []*regexp.Regexp{
		regexp.MustCompile(`<a[^>]+href=["']([^"']+)["']`),
		regexp.MustCompile(`<link[^>]+href=["']([^"']+)["']`),
		regexp.MustCompile(`<iframe[^>]+src=["']([^"']+)["']`),
		regexp.MustCompile(`<img[^>]+src=["']([^"']+)["']`),
		regexp.MustCompile(`<form[^>]+action=["']([^"']+)["']`),
	}

	for _, pattern := range patterns {
		matches := pattern.FindAllStringSubmatch(html, -1)
		for _, match := range matches {
			if len(match) > 1 && match[1] != "" {
				link := match[1]
				if !seen[link] {
					seen[link] = true
					links = append(links, link)
				}
			}
		}
	}

	return links
}

// –æ–±—Ä–µ–∑–∞–µ–º URL –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞
func truncateURL(url string, maxLen int) string {
	if len(url) <= maxLen {
		return url
	}
	return url[:maxLen-3] + "..."
}

// CrawlMultiple –∑–∞–ø—É—Å–∫–∞–µ—Ç –æ–±—Ö–æ–¥ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –Ω–∞—á–∞–ª—å–Ω—ã—Ö URL
func (c *BFSCrawler) CrawlMultiple(startURLs []string) ([]Result, error) {
	opts := DefaultNormalizeOptions()

	// –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ URL –≤ –æ—á–µ—Ä–µ–¥—å
	for _, url := range startURLs {
		canonical := CanonicalizeURL(url, opts)
		if !c.visited[canonical] {
			c.queue = append(c.queue, CrawlNode{URL: url, Depth: 0})
			c.visited[canonical] = true
		}
	}

	fmt.Println("\nüîÑ Starting BFS crawl...")
	fmt.Printf("üìä Seeds: %d | Max depth: %d | Rate: %v | Max URLs: %d\n\n",
		len(startURLs), c.config.MaxDepth, c.config.RateLimit, c.config.MaxURLs)
	
	visitedCount := 0

	for len(c.queue) > 0 && visitedCount < c.config.MaxURLs {
		current := c.queue[0]
		c.queue = c.queue[1:]

		if current.Depth > c.config.MaxDepth {
			continue
		}

		visitedCount++
		fmt.Printf("\rüåê Visiting [%d/%d, depth %d]: %s   ",
			visitedCount, c.config.MaxURLs, current.Depth, truncateURL(current.URL, 60))

		links, status := c.fetchAndExtractLinks(current.URL)

		if status > 0 {
			c.mu.Lock()
			c.allURLs = append(c.allURLs, Result{
				URL:        current.URL,
				StatusCode: status,
				IsSPARoute: false,
			})
			c.mu.Unlock()
		}

		if current.Depth < c.config.MaxDepth {
			for _, link := range links {
				abs, err := ToAbsoluteURL(link, current.URL)
				if err != nil || abs == "" {
					continue
				}

				if !IsInScope(abs, c.scanner.BaseURL, "/") {
					continue
				}

				canonical := CanonicalizeURL(abs, opts)

				c.mu.Lock()
				if !c.visited[canonical] {
					c.visited[canonical] = true
					c.queue = append(c.queue, CrawlNode{
						URL:   abs,
						Depth: current.Depth + 1,
					})
				}
				c.mu.Unlock()
			}
		}

		if c.config.RateLimit > 0 {
			time.Sleep(c.config.RateLimit)
		}
	}

	fmt.Println("\n\n‚úÖ BFS crawl complete!")
	return c.allURLs, nil
}
